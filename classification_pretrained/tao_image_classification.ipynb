{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pretrained image model from Nvidia TAO model zoo on STM32 platforms with STEdgeAI Developer Cloud\n",
    "\n",
    "This notebook provides an example of using a pretrained image-classification model from ngc model zoo, optimize and adapt it to deploy and benchmark on STM32 targets.\n",
    "\n",
    "The scripts below let you download and use one of the pretrained models from pretrained [classification model repository of Nvidia TAO](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pretrained_classification/version). The models come under [creative commons license](https://creativecommons.org/licenses/by/4.0/) as mentioned on the [explainability](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pretrained_classification/explainability) link for these models. The notebook in this folder provides a demo example of how a user can download mobilenet_v2 model from the repository, convert it to onnx, and then quantize it, and finally, benchmarking using [STEdgeAI Developer Cloud](https://stm32ai.st.com/stm32-cube-ai-dc/).pretrained image classification mdoel from the  complete life cycle of the model training, optimization and benchmarking using [NVIDIA TAO Toolkit](https://developer.nvidia.com/tao-toolkit) and [STEdgeAI Developer Cloud](https://stm32ai.st.com/stm32-cube-ai-dc/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "This software component is licensed by ST under BSD-3-Clause license,\n",
    "the \"License\"; \n",
    "\n",
    "You may not use this file except in compliance with the\n",
    "License. \n",
    "\n",
    "You may obtain a copy of the License at: https://opensource.org/licenses/BSD-3-Clause\n",
    "\n",
    "Copyright (c) 2024 STMicroelectronics. All rights reserved.\n",
    "\n",
    "Copyright (c) 2024 Nvidia. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) A. Set proxy variables if working behind corporate proxies.\n",
    "\n",
    "The following section sets the proxies and ssl verification flag when the users are working behind the proxies. This setup is necessary to be able to communicate with internet.\n",
    "\n",
    "Replace the `userName`, `password`, and `proxy_port` with your correct username, password and proxy port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set proxies\n",
    "import os\n",
    "# os.environ[\"http_proxy\"]='http://userName:password.@example.com:proxy_port'\n",
    "# os.environ[\"https_proxy\"] = 'http://userName:password.@example.com:proxy_port'\n",
    "# os.environ[\"NO_SSL_VERIFY\"]=\"1\"\n",
    "# os.environ[\"SSL_VERIFY\"]=\"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf2onnx==1.14.0\n",
    "!pip install opencv_python==4.6.0.66\n",
    "!pip install onnxruntime==1.14.1\n",
    "!pip install onnx==1.12.0\n",
    "!pip install matplotlib==3.3.3\n",
    "!pip install tensorflow==2.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions to make the code more modular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import onnxruntime as ort\n",
    "import json\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "def get_label_from_class_map_file(class_id:int,\n",
    "                                  json_file_path:str ='classmap.json'):\n",
    "    \"\"\"\n",
    "    Get the class label given the path to the class map JSON file and the class ID.\n",
    "\n",
    "    Args:\n",
    "    class_id (int): The ID of the class.\n",
    "    json_file_path (str): Path to the class map JSON file.\n",
    "\n",
    "    Returns:\n",
    "    str: The label of the class corresponding to the given ID.\n",
    "    \"\"\"\n",
    "    # Load the class map from the JSON file\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        class_map = json.load(f)\n",
    "\n",
    "    # Create a reverse mapping from IDs to class labels\n",
    "    id_to_class_map = {v: k for k, v in class_map.items()}\n",
    "\n",
    "    # Retrieve and return the class label for the given ID\n",
    "    return id_to_class_map.get(class_id, \"Unknown ID\")\n",
    "\n",
    "\n",
    "def load_image(image_path:str,\n",
    "               input_shape:tuple=(224,224)):\n",
    "    \"\"\"\n",
    "    Load the image in 'bgr' mode, and prepare it for the inference of the model\n",
    "    This will include the image loading as pixel values between 0-255 in bgr mode\n",
    "    subtract the mean values for these channel to make the images zero centered\n",
    "    add a dimension for batch and finally transpose the images to have channel first\n",
    "    Args:\n",
    "    image_path (str): path to the image file\n",
    "    input_shape (integer tuple): shape of the image after resize\n",
    "\n",
    "    Returns:\n",
    "    np array of 4D: The preprocessed image\n",
    "    \"\"\"\n",
    "    channel_means = [np.float32(103.939), np.float32(116.779), np.float32(123.68)]\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path)  # Load image in BGR format\n",
    "    image_resized = cv2.resize(image, input_shape) # Resize to input_shape\n",
    "    image_resized = image_resized - channel_means # changing pixels to zero-mean values\n",
    "    \n",
    "    # Convert image to float32 \n",
    "    input_image = image_resized.astype(np.float32) \n",
    "\n",
    "    # Add a batch dimension (b, h, w, c)\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "    # Transpose to (1, C, H, W) if the model expects channels first\n",
    "    input_image = np.transpose(input_image, (0, 3, 1, 2))\n",
    "    return input_image\n",
    "\n",
    "\n",
    "def run_prediction(model_path, input_image):\n",
    "    \"\"\"\n",
    "    Load the model and run the inference to get the output of the inference\n",
    "    \n",
    "    Args:\n",
    "    model_path (str): path to the model file (the supported formats are hdf5 or onnx)\n",
    "    input_image (np.array): the preprocessed image\n",
    "\n",
    "    Returns:\n",
    "    np array(s) of the predictions\n",
    "    \"\"\"\n",
    "    if model_path.split('.')[-1] == 'onnx':\n",
    "        try:\n",
    "            # Load the ONNX model\n",
    "            session = ort.InferenceSession(model_path)\n",
    "            # Get model input name\n",
    "            input_name = session.get_inputs()[0].name\n",
    "            # Run inference\n",
    "            return np.asarray(session.run(None, {input_name: input_image})).squeeze()\n",
    "        except:\n",
    "            print('Error occured while performing model inference!\\nProblem with model file or input format!')\n",
    "            return None\n",
    "    elif model_path.split('.')[-1] == 'hdf5':\n",
    "        try:\n",
    "            # load model and run inference\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            return model.predict(input_image)\n",
    "        except:\n",
    "            print('Error occured while performing model inference!\\nProblem with model file or input format!')\n",
    "            return None\n",
    "    else:\n",
    "        print('model is not supported!')\n",
    "        return None\n",
    "\n",
    "def download_file(url:str, save_path:str):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL and saves it to the specified location.\n",
    "\n",
    "    Args:\n",
    "    url (str): URL of the file to be downloaded\n",
    "    save_path (str): Path where the file should be saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a HTTP request to the URL\n",
    "        response = requests.get(url, stream=True, verify=False)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "        # Open the file in write-binary mode and write the content\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "\n",
    "        print(f\"File downloaded successfully and saved to {save_path}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while downloading the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the pretrained model\n",
    "\n",
    "This notebook will use [`mobilenet_v2`](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pretrained_classification/files?version=mobilenet_v2) as an example but users can use any other model from the [repositry](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pretrained_classification/version). \n",
    "\n",
    "__NOTE__: In case a different mode is used all the sections of notebook have to be adapted to have valid file paths and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/pretrained_classification/mobilenet_v2/files?redirect=true&path=mobilenet_v2.hdf5'\n",
    "model_save_path = 'mobilenet_v2.hdf5'\n",
    "download_file(url=model_url, save_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the downloaded model to onnx format\n",
    "\n",
    "The downloaded pretrained models will work as it is for the GPU machines but can not be used with the tensorflow 2.x on the CPU machines. To make them cross-compatible we will convert the downloaded model to onnx format in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "\n",
    "input_model_path = './mobilenet_v2.hdf5'\n",
    "opset_version = 15  # You can change this to the desired opset version\n",
    "output_model_path = \"mobilenet_v2.onnx\"\n",
    "\n",
    "# Load your pretrained model from the H5 file\n",
    "model = tf.keras.models.load_model(input_model_path)\n",
    "\n",
    "# Define the input signature\n",
    "spec = (tf.TensorSpec(model.inputs[0].shape, tf.float32, name=\"input\"),)\n",
    "\n",
    "# Convert the model to ONNX format\n",
    "tf2onnx.convert.from_keras(model, input_signature=spec, opset=opset_version, output_path=output_model_path)\n",
    "print(f\"Model has been converted to ONNX format with opset version {opset_version} and saved to {output_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download some test images from Open Image dataset to perform the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#create the test_images directory\n",
    "test_image_dir = './test_images/'\n",
    "if not os.path.isdir(test_image_dir):\n",
    "    os.makedirs(test_image_dir)\n",
    "\n",
    "# loading the csv file with the url and labels of the images and downloading the images one by one\n",
    "image_urls = pd.read_csv('sample_image_paths.csv',header=0)\n",
    "for i in range(len(image_urls)):\n",
    "    download_file(url=image_urls.iloc[i].url,save_path= os.path.join(test_image_dir, image_urls.iloc[i].label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on the converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = 'mobilenet_v2.onnx'\n",
    "test_image_path = './test_images/cookie.jpg'\n",
    "input_shape=(224,224) # open the downloaded model in netron to confirm the shape if different from 224x224\n",
    "\n",
    "input_image = load_image(test_image_path, input_shape=input_shape)\n",
    "\n",
    "# run prediction on the onnx model\n",
    "result = run_prediction(model_path=onnx_model_path, input_image=input_image)\n",
    "\n",
    "print(f'prediction on {test_image_path}')\n",
    "print(f'\\tconfidence : {np.max(result)}\\n\\tlabel : {get_label_from_class_map_file(np.argmax(result))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantizing the onnx model using QDQ post-training quantization\n",
    "The following cell quantize the onnx model into QDQ format so that it is more optimized and better suited for EdgeAI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize the model\n",
    "import sys\n",
    "sys.path.append('../classification_tf2/')\n",
    "import onnx_utils\n",
    "onnx_utils.quantize_onnx_model(input_model = './mobilenet_v2.onnx', \n",
    "                              calibration_dataset_path = './test_images/',\n",
    "                              preproc_mode='caffe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = 'mobilenet_v2_QDQ_quant.onnx'\n",
    "test_image_path = './test_images/cookie.jpg'\n",
    "\n",
    "input_shape=(224,224) # open the downloaded model in netron to confirm the shape if different from 224x224\n",
    "\n",
    "input_image = load_image(test_image_path, input_shape=input_shape)\n",
    "\n",
    "# run prediction on the onnx model\n",
    "result = run_prediction(model_path=onnx_model_path, input_image=input_image)\n",
    "\n",
    "print(f'prediction on {test_image_path}')\n",
    "print(f'\\tconfidence : {np.max(result)}\\n\\tlabel : {get_label_from_class_map_file(np.argmax(result))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking the optimized model using STEdgeAI Developer Cloud\n",
    "\n",
    "Getting the package for connecting to STEdgeAI Developer Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gitdir\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: README.md\n",
      "Downloaded: __init__.py\n",
      "Downloaded: benchmark_service.py\n",
      "Downloaded: cloud.py\n",
      "Downloaded: endpoints.py\n",
      "Downloaded: file_service.py\n",
      "Downloaded: generate_nbg_service.py\n",
      "Downloaded: helpers.py\n",
      "Downloaded: login_service.py\n",
      "Downloaded: stm32ai_service.py\n",
      "Downloaded: user_service.py\n",
      "Downloaded: errors.py\n",
      "Downloaded: stm32ai.py\n",
      "Downloaded: types.py\n",
      "\n",
      "âœ” Download complete\n"
     ]
    }
   ],
   "source": [
    "!gitdir https://github.com/STMicroelectronics/stm32ai-modelzoo-services/tree/main/common/stm32ai_dc\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Reorganize local folders\n",
    "if os.path.exists('./stm32ai_dc'):\n",
    "    shutil.rmtree('./stm32ai_dc')\n",
    "shutil.move('./common/stm32ai_dc', './stm32ai_dc')\n",
    "shutil.rmtree('./common')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import, helper and UI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from stm32ai_dc import (CliLibraryIde, CliLibrarySerie, CliParameters, MpuParameters, MpuEngine, AtonParameters,\n",
    "                        CloudBackend, Stm32Ai)\n",
    "\n",
    "sys.path.append(os.path.abspath('stm32ai'))\n",
    "os.environ['STATS_TYPE'] = 'stm32ai_tao'\n",
    "\n",
    "# create a directory for outputs for stm32ai developer cloud operations\n",
    "stm32ai_output_dir = './results_mobilenetv2/stm32ai_outputs'\n",
    "os.makedirs(stm32ai_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_mpu_options(board_name: str = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Get MPU benchmark options depending on MPU board selected\n",
    "    Each MPU board has different settings,\n",
    "    i.e. different number of cpu_cores or engine (CPU only or HW_Accelerator also)\n",
    "    Input:\n",
    "        board_name:str, name of the mpu board\n",
    "    Returns:\n",
    "        tuple: engine_used and num_cpu_cores.\n",
    "    \"\"\"\n",
    "\n",
    "    #define configuration by MPU board\n",
    "    STM32MP257F_EV1 = {\n",
    "        \"engine\": MpuEngine.HW_ACCELERATOR,\n",
    "        \"cpu_cores\": 2\n",
    "    }\n",
    "\n",
    "    STM32MP157F_DK2 = {\n",
    "        \"engine\": MpuEngine.CPU,\n",
    "        \"cpu_cores\": 2\n",
    "    }\n",
    "\n",
    "    STM32MP135F_DK = {\n",
    "        \"engine\": MpuEngine.CPU,\n",
    "        \"cpu_cores\": 1\n",
    "    }\n",
    "\n",
    "    #recover parameters based on board name:\n",
    "    if board_name == \"STM32MP257F-EV1\":\n",
    "        engine_used = STM32MP257F_EV1.get(\"engine\")\n",
    "        num_cpu_cores = STM32MP257F_EV1.get(\"cpu_cores\")\n",
    "    elif board_name == \"STM32MP157F-DK2\":\n",
    "        engine_used = STM32MP157F_DK2.get(\"engine\")\n",
    "        num_cpu_cores = STM32MP157F_DK2.get(\"cpu_cores\")\n",
    "    elif board_name == \"STM32MP135F-DK\":\n",
    "        engine_used = STM32MP135F_DK.get(\"engine\")\n",
    "        num_cpu_cores = STM32MP135F_DK.get(\"cpu_cores\")\n",
    "    else :\n",
    "        engine_used = MpuEngine.CPU\n",
    "        num_cpu_cores = 1\n",
    "\n",
    "    return engine_used, num_cpu_cores\n",
    "\n",
    "def analyze_footprints(report: object = None) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes the memory footprint of a STEdgeAI model.\n",
    "\n",
    "    Args:\n",
    "        report: A report object containing information about the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    activations_ram: float = report.ram_size / 1024\n",
    "    runtime_ram: float = report.estimated_library_ram_size / 1024\n",
    "    total_ram: float = activations_ram + runtime_ram\n",
    "    weights_rom: float = report.rom_size / 1024\n",
    "    code_rom: float = report.estimated_library_flash_size / 1024\n",
    "    total_flash: float = weights_rom + code_rom\n",
    "    macc: float = report.macc / 1e6\n",
    "    print(\"[INFO] : STEdgeAI model memory footprint\")\n",
    "    print(\"[INFO] : MACCs : {} (M)\".format(macc))\n",
    "    print(\"[INFO] : Total Flash : {0:.1f} (KiB)\".format(total_flash))\n",
    "    print(\"[INFO] :     Flash Weights  : {0:.1f} (KiB)\".format(weights_rom))\n",
    "    print(\"[INFO] :     Estimated Flash Code : {0:.1f} (KiB)\".format(code_rom))\n",
    "    print(\"[INFO] : Total RAM : {0:.1f} (KiB)\".format(total_ram))\n",
    "    print(\"[INFO] :     RAM Activations : {0:.1f} (KiB)\".format(activations_ram))\n",
    "    print(\"[INFO] :     RAM Runtime : {0:.1f} (KiB)\".format(runtime_ram))\n",
    "\n",
    "\n",
    "def benchmark_model(stmai:object,\n",
    "                    model_path:str,\n",
    "                    model_name:str,\n",
    "                    optimization:str,\n",
    "                    from_model:str,\n",
    "                    board_name:str,\n",
    "                    allocateInputs:bool =True,\n",
    "                    allocateOutputs:bool=True) -> float:\n",
    "    \"\"\"\n",
    "    Benchmarks the give model to calculate the footprint on a STM32 Target board.\n",
    "\n",
    "    Args:\n",
    "        stmai:object, an object of stm32ai_dc\n",
    "        model_path:str, path to the model file\n",
    "        model_name:str, path to the model file\n",
    "        optimization:str, the way model is to be optimized available options ['balanced', 'time', 'ram']\n",
    "        from_model:str, if the model is coming from zoo or is a custom model from the user\n",
    "        board_name:str, target board name from one of the available boards on the dev cloud\n",
    "        allocateInputs:bool, If set to true activations buffer will be also used to handle the input buffers. \n",
    "        allocateOutputs:bool, If set to \"True\", activations buffer will be also used to handle the output buffers.\n",
    "\n",
    "    Returns:\n",
    "        fps: frames per second (1/inference_time)\n",
    "    \"\"\"\n",
    "    print(f\"Benchmarking on: {board_name}\")\n",
    "    if \"mp\" in board_name.lower():\n",
    "        # if mpu is selected as the target\n",
    "        model_extension = os.path.splitext(model_path)[1]\n",
    "        # only supported options are quantized tflite or onnx models\n",
    "        if model_extension in ['.onnx', '.tflite']:\n",
    "            if \"stm32mp2\" in board_name.lower(): # if mp2 is selected as the target board optimize the model to generate a .nbg file\n",
    "                optimized_model_path = os.path.dirname(model_path) + \"/\"\n",
    "                try:\n",
    "                    stmai.upload_model(model_path)\n",
    "                    model = model_name\n",
    "                    res = stmai.generate_nbg(model)\n",
    "                    stmai.download_model(res, optimized_model_path + res)\n",
    "                    model_path=os.path.join(optimized_model_path,res)\n",
    "                    nb_model_name = os.path.splitext(os.path.basename(model_path))[0] + \".nb\"\n",
    "                    rename_model_path=os.path.join(optimized_model_path,nb_model_name)\n",
    "                    os.rename(model_path, rename_model_path)\n",
    "                    model_path = rename_model_path\n",
    "                    print(\"[INFO] : Optimized Model Name:\", model_name)\n",
    "                    print(\"[INFO] : Optimization done ! Model available at :\",optimized_model_path)\n",
    "                    model_name = nb_model_name\n",
    "                except Exception as e:\n",
    "                    print(f\"[FAIL] : Model optimization via Cloud failed : {e}.\")\n",
    "                    print(\"[INFO] : Use default model instead of optimized ...\")\n",
    "        else:\n",
    "            print(\"[ERROR]: Only .tflite or .onnx models can be benchmarked for MPU\")\n",
    "            fps = 0\n",
    "            return fps\n",
    "\n",
    "        engine, nbCores = get_mpu_options(board_name)\n",
    "        stmai_params = MpuParameters(model=model_name,\n",
    "                                     nbCores=nbCores,\n",
    "                                     engine=engine)\n",
    "\n",
    "    elif 'stm32n6' in board_name.lower():\n",
    "        model_extension = os.path.splitext(model_path)[1]\n",
    "        if model_extension in ['.onnx', '.tflite']:\n",
    "            stmai_params = CliParameters(model=model_name,\n",
    "                                         target='stm32n6',\n",
    "                                         stNeuralArt='default',\n",
    "                                         atonnOptions=AtonParameters())\n",
    "        else:\n",
    "            print(\"[ERROR]: Only .tflite or .onnx models can be benchmarked for N6\")\n",
    "        \n",
    "    else:\n",
    "        # target board in mcu, prepare stm32ai parameters\n",
    "        stmai_params = CliParameters(model=model_name,\n",
    "                                     optimization=optimization,\n",
    "                                     allocateInputs=allocateInputs,\n",
    "                                     allocateOutputs=allocateOutputs,\n",
    "                                     fromModel=from_model)\n",
    "    # running the benchmarking with prepared params\n",
    "    benchmark_report_dir = f'{stm32ai_output_dir}/benchmark_reports/'\n",
    "    os.makedirs(benchmark_report_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    try:\n",
    "        result = stmai.benchmark(stmai_params, board_name)\n",
    "        fps = analyze_inference_time(report=result,\n",
    "                                     target_mpu=\"mp\" in board_name.lower())\n",
    "        # Save the result in outputs folder\n",
    "        with open(f'./{benchmark_report_dir}/{model_name}_{board_name}.txt', 'w') as file_benchmark:\n",
    "            file_benchmark.write(f'{result}')\n",
    "        return fps\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Benchmarking failed on board: {board_name}\")\n",
    "        fps = 0\n",
    "        return fps\n",
    "\n",
    "def analyze_inference_time(report: object = None,\n",
    "                           target_mpu = False) -> float:\n",
    "    \"\"\"\n",
    "    Analyzes the inference time of a STEdgeAI model, prints the report and return the FPS.\n",
    "    Args:\n",
    "        report: A report object containing information about the model.\n",
    "        target_mpu: a boolean (True: if target is MPU, False: otherwise)\n",
    "\n",
    "    Returns:\n",
    "        The frames per second (FPS) of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    inference_time: float = report.duration_ms\n",
    "    fps: float = 1000.0/inference_time\n",
    "    if not target_mpu:\n",
    "        # in mpu benchmark result report we do not have cycles\n",
    "        cycles: int = report.cycles\n",
    "        print(\"[INFO] : Number of cycles : {} \".format(cycles))\n",
    "    print(\"[INFO] : Inference Time : {0:.1f} (ms)\".format(inference_time))\n",
    "    print(\"[INFO] : FPS : {0:.1f}\".format(fps))\n",
    "    return fps\n",
    "\n",
    "\n",
    "# UI widgets\n",
    "\n",
    "# optimization options\n",
    "optimization: List[str] = [\"balanced\", \"time\", \"ram\"]\n",
    "optim_dropdown: widgets.Dropdown = widgets.Dropdown(\n",
    "    options=optimization,\n",
    "    value=optimization[0],\n",
    "    description='Optim:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# STM32MCU series for code generation target\n",
    "series_name: List[str] = [\n",
    "    \"STM32H7\", \"STM32F7\", \"STM32F4\", \"STM32L4\", \"STM32G4\",\n",
    "    \"STM32F3\", \"STM32U5\", \"STM32L5\", \"STM32F0\", \"STM32L0\",\n",
    "    \"STM32G0\", \"STM32C0\", \"STM32WL\", \"STM32H5\"\n",
    "]\n",
    "series_dropdown: widgets.Dropdown = widgets.Dropdown(\n",
    "    options=series_name,\n",
    "    value=series_name[0],\n",
    "    description='Series:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# options for the IDE while code generation\n",
    "IDE_name: List[str] = [\"gcc\", \"iar\", \"keil\"]\n",
    "ide_dropdown: widgets.Dropdown = widgets.Dropdown(\n",
    "    options=IDE_name,\n",
    "    value=IDE_name[0],\n",
    "    description='IDE:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Login to STEdgeAI Developer Cloud\n",
    "Set environment variables with your credentials to acces STEdgeAI Developer Cloud.\n",
    "\n",
    "If you don't have an account yet go to: https://stm32ai-cs.st.com/home and click on sign in to create an account. \n",
    "\n",
    "Then set the environment variables below with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "# Set environment variables with your credentials to access \n",
    "# STEdgeAI Developer Cloud services\n",
    "# Fill the username with your login address \n",
    "username = 'your.email@example.com'\n",
    "os.environ['stmai_username'] = username\n",
    "print('Enter you password')\n",
    "password = getpass.getpass()\n",
    "os.environ['stmai_password'] = password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in STEdgeAI Developer Cloud \n",
    "try:\n",
    "    stmai = Stm32Ai(CloudBackend(str(username), str(password)))\n",
    "    print(\"Successfully Connected!\")\n",
    "except Exception as e:\n",
    "    print(\"Error: please verify your credentials\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Upload the model on STEdgeAI Developer Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenet_v2'\n",
    "model_path = './mobilenet_v2_QDQ_quant.onnx'\n",
    "model_name = os.path.basename(model_path)\n",
    "from_model = 'user'\n",
    "\n",
    "try:\n",
    "  stmai.upload_model(model_path)\n",
    "  print(f'Model {model_name} is uploaded !')\n",
    "except Exception as e:\n",
    "    print(\"ERROR: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Select the STEdgeAI optimization setting\n",
    "| Configuration | Description |\n",
    "| --- | --- |\n",
    "| balanced | default compromise between RAM footprint and latency. |\n",
    "| time | optimize for latency. |\n",
    "| ram | optimize for minimal RAM footprint. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(optim_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Analyze your model memory footprints for STM32MCU targets\n",
    "When analyzing the footprints of the model for STM32MCU targets, following parameters can be configured for stm32ai.analyze callback:\n",
    "\n",
    "CLIParameters (options of STEdgeAI):\n",
    "\n",
    "| Parameter | Description |\n",
    "| --- | --- |\n",
    "| model | Model name corresponding to the file name uploaded. This parameter is __required__. |\n",
    "| optimization | Optimization setting: \"balanced\", \"time\" or \"ram\". This parameter is __required__. |\n",
    "| allocateInputs | If set to \"True\", activations buffer will be also used to handle the input buffers. This parameter is __optional__. Default value is \"True\". |\n",
    "| allocateOutputs | If set to \"True\", activations buffer will be also used to handle the output buffers. This parameter is __optional__. Default value is \"True\". |\n",
    "| noOnnxOptimizer | If set to \"True\", allows to disable the ONNX optimizer pass. This parameter is __optional__. Default value is \"False\". |\n",
    "| fromModel | To identify the origin model when coming from ST model zoo. This parameter is __optional__. Default value is \"user\".|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze RAM/Flash model memory footprints after optimization by STEdgeAI\n",
    "optimization = optim_dropdown.value\n",
    "print(f'Anlyzing model : {model_name}, using opimization : {optimization}')\n",
    "# The runtime library footprint varies slightly depending on the STM32 series\n",
    "# For an estimation, we use the default series to the STM32F4\n",
    "try:\n",
    "  result = stmai.analyze(CliParameters(model=model_path,\n",
    "                                       optimization=optimization,\n",
    "                                       allocateInputs=True,\n",
    "                                       allocateOutputs=True,\n",
    "                                       fromModel=from_model))\n",
    "  # analyze and print the summary of footprint report\n",
    "  analyze_footprints(report=result)\n",
    "  \n",
    "  # Save the result in outputs folder\n",
    "  stm32ai_analysis_dir = f'{stm32ai_output_dir}/analysis_report'\n",
    "  os.makedirs(stm32ai_analysis_dir, exist_ok=True)\n",
    "  with open(f'./{stm32ai_analysis_dir}/{model_name}_analyze.txt', 'w') as file_analyze:\n",
    "    file_analyze.write(f'{result}')\n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Benchmark your model on a STM32 target\n",
    "Starting from STEdgeAI dev cloud version 10.0.0 onwards, the models can be benchmarked for STM32MCU and STM32MPU as well as for STM32NPU target boards.\n",
    "\n",
    "Here's a table with the parameters and their descriptions while benchmarking for the STM32MCU targets (CLIParameters options of STEdgeAI):\n",
    "\n",
    "| Parameter | Description |\n",
    "| --- | --- |\n",
    "| model | Model name corresponding to the file name uploaded. This parameter is required. |\n",
    "| optimization | Optimization setting: \"balanced\", \"time\" or \"ram\". This parameter is required. |\n",
    "| allocateInputs | If set to \"True\", activations buffer will be also used to handle the input buffers. This parameter is optional. Default value is \"True\". |\n",
    "| allocateOutputs | If set to \"True\", activations buffer will be also used to handle the output buffers. This parameter is optional. Default value is \"True\". |\n",
    "| noOnnxOptimizer | If set to \"True\", allows to disable the ONNX optimizer pass. This parameter is optional. Default value is \"False\". Apply only to ONNX file will be ignored otherwise. |\n",
    "| fromModel | To identify the origin model when coming from ST model zoo. This parameter is optional. Default value is \"user\". |\n",
    "\n",
    "\n",
    "While for the STM32MPU targets, only needed parameters are:\n",
    "\n",
    "| Parameter | Description |\n",
    "| --- | --- |\n",
    "| model | Model name corresponding to the file name uploaded. This parameter is __required__. |\n",
    "| nbCores | Number of CPU cores used for benchmarking. This parameter is __set by the code__ depending on the type of MPU. The value should be an integer \"1\", or \"2\". |\n",
    "| engine | Choice of the hardware engine used on the board for benchmarking.This parameter is __set by the code__ depending on the target MPU. For STM32MP1X boards it is \"MpuEngine.CPU\" and for STM32MP2X this is \"MpuEngine.HW_ACCELERATOR\". |\n",
    "\n",
    "* Note that the the code section below, the boad_name to benchmark the model on should be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the available board on STEdgeAI Developer Cloud\n",
    "boards = stmai.get_benchmark_boards()\n",
    "board_names = [boards[i].name for i in range(len(boards))]\n",
    "print(\"Available boards:\", board_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1. Benchmark on all available STM32 boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the model on all STEdgeAI Developer Cloud boards\n",
    "print(model_name)\n",
    "fps_array = []\n",
    "# loop through all boards\n",
    "for board_name in board_names:\n",
    "        fps_array.append(benchmark_model(stmai=stmai,\n",
    "                                         model_path=model_path,\n",
    "                                         model_name=model_name,\n",
    "                                         optimization=optimization,\n",
    "                                         from_model=from_model,\n",
    "                                         board_name=board_name,\n",
    "                                         allocateInputs= True,\n",
    "                                         allocateOutputs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Frame per Second benchmark\n",
    "sorted_fps = sorted(fps_array, reverse=True)\n",
    "sorted_boards = [board_names[fps_array.index(i)] for i in sorted_fps]\n",
    "fig = plt.figure(1, figsize=(15, 8), tight_layout=True)\n",
    "# colors = sns.color_palette()\n",
    "\n",
    "colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974',\n",
    "          '#64B5CD', '#B4A7D6', '#AEC7E8', '#FFA07A', '#FFC0CB',\n",
    "          '#FFFFB3', '#8DD3C7', '#BEBADA', '#FDB462', '#FB8072',\n",
    "          '#FF6347', '#4682B4', '#6A5ACD', '#7FFF00', '#D2691E']\n",
    "\n",
    "plt.bar(sorted_boards, sorted_fps, color=colors[:len(boards)], width=0.7)\n",
    "plt.ylabel('FPS', fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(sorted_boards, rotation = 75)\n",
    "plt.title('STM32 FPS benchmark')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2. Benchmark on a selected board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a board among the available boards\n",
    "board_dropdown = widgets.Dropdown(\n",
    "    options = board_names,\n",
    "    value = 'STM32N6570-DK',\n",
    "    description ='Board:',\n",
    "    disabled = False,)\n",
    "\n",
    "display(board_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_name = board_dropdown.value\n",
    "print(model_name, board_name)\n",
    "fps = benchmark_model(stmai=stmai,\n",
    "                      model_path=model_path,\n",
    "                      model_name=model_name,\n",
    "                      optimization=optimization,\n",
    "                      from_model=from_model,\n",
    "                      board_name=board_name,\n",
    "                      allocateInputs= True,\n",
    "                      allocateOutputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Generate your model optimized C code for STM32MCU targets\n",
    "\n",
    "To deploy the model on an STM32MCU target the user has to generate the C-Code of the optimized model. Here's a table with the parameters and their descriptions for the stm32.generate callback (CLIParameters of STEdgeAI):\n",
    "\n",
    "| Parameter | Description |\n",
    "| --- | --- |\n",
    "| model | Model name corresponding to the file name uploaded. This parameter is required. |\n",
    "| optimization | Optimization setting: \"balanced\", \"time\" or \"ram\". This parameter is required. |\n",
    "| allocateInputs | If set to \"True\", activations buffer will be also used to handle the input buffers. This parameter is optional. Default value is \"True\". |\n",
    "| allocateOutputs | If set to \"True\", activations buffer will be also used to handle the output buffers. This parameter is optional. Default value is \"True\". |\n",
    "| noOnnxOptimizer | If set to \"True\", allows to disable the ONNX optimizer pass. This parameter is optional. Default value is \"False\". Apply only to ONNX file will be ignored otherwise. |\n",
    "| includeLibraryForSerie | Include the runtime library for the given STM32 series. This parameter is optional. |\n",
    "| fromModel | To identify the origin model when coming from ST model zoo. This parameter is optional. |\n",
    "\n",
    "\n",
    "\n",
    "### NOTE\n",
    "\n",
    "There is no need for this step if the deployment is intended on the MPU. One can directly deploy the .tflite model on the STM32MPUs. In case of STM32MP2x, an optimized version of the model should be already available in the path where the starting model was placed with the same name as model and extension \".nb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(series_dropdown)\n",
    "display(ide_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = series_dropdown.value\n",
    "IDE = ide_dropdown.value\n",
    "print(f'Generating optimized C code of {model_name} model, for {series} series boards!\\n')\n",
    "# Generate model .c/.h code + Lib/Inc on STEdgeAI Developer Cloud\n",
    "stm32ai_code_dir = f'{stm32ai_output_dir}/generated_code'\n",
    "os.makedirs(stm32ai_code_dir, exist_ok=True)\n",
    "result = stmai.generate(CliParameters(\n",
    "    model=model_name,\n",
    "    output=stm32ai_code_dir,\n",
    "    optimization=optimization,\n",
    "    allocateInputs=True,\n",
    "    allocateOutputs=True,\n",
    "    includeLibraryForSerie=CliLibrarySerie(series),\n",
    "    includeLibraryForIde=CliLibraryIde(IDE),\n",
    "    fromModel=from_model\n",
    "))\n",
    "!ls \"{stm32ai_code_dir}\"\n",
    "# print 20 first lines of the report\n",
    "if os.path.isfile(f'./{stm32ai_code_dir}/network_generate_report.txt'):\n",
    "  print(\"\\n\\n---- code generation report ----\\n\",\"*\" * 80)\n",
    "  with open(f'./{stm32ai_code_dir}/network_generate_report.txt', 'r') as f:\n",
    "    for _ in range(20): print(next(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You are ready to integrate your model in your STM32 application !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) : Delete your model from your STEdgeAI Developer Cloud space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stmai.delete_model(model_name):\n",
    "    print(f'{model_name} deleted from STEdgeAI developer Cloud workspace!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment on STM32N6, STM32H7* and STM32MPU\n",
    "\n",
    "The `QDQ` quantized models can be deployed using the [stm32ai-modelzoo-services](https://github.com/STMicroelectronics/stm32ai-modelzoo-services) as an [image_classification](https://github.com/STMicroelectronics/stm32ai-modelzoo-services/tree/main/image_classification) model. For knowing more details on how to do that please refer to [Deploying Image Classification models on STM32MCU](https://github.com/STMicroelectronics/stm32ai-modelzoo-services/blob/main/image_classification/deployment/README.md) and [Deploying Image Classificaiton Models on STM32MPU](https://github.com/STMicroelectronics/stm32ai-modelzoo-services/blob/main/image_classification/deployment/README_MPU.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
