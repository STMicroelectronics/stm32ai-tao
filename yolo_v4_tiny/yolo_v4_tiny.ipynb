{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAO Object Detection using Nvidia TAO YOLOv4 Tiny with STEdgeAI Devloper Cloud\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "This notebook provides a complete life cycle of an object detection model training, optimization and benchmarking using [NVIDIA TAO Toolkit](https://developer.nvidia.com/tao-toolkit) and [STEdgeAI Developer Cloud](https://stm32ai.st.com/stm32-cube-ai-dc/).\n",
    "\n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "[STEdgeAI Developer Cloud](https://stm32ai-cs.st.com/home) is a free-of-charge online platform and services allowing the creation, optimization, benchmarking, and generation of AI models for the STM32 microcontrollers. It is based on the [STEdgeAI](https://stm32ai.st.com/stm32-cube-ai/) core technology.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"float: center;background-color: white; width: 1080\" src=\"../docs/TAO-STM32CubeAI.png\" width=\"1080\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample prediction of YOLOv4-tiny\n",
    "<br>\n",
    "\n",
    "<img style=\"float: center;background-color: white; width: 1080\" src=\"../docs/sample_prediction.jpg\" width=\"1080\">\n",
    "\n",
    "<br> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained model and train a YOLO v4 Tiny model on the Coco2017 Person dataset (a subset of Coco2017)\n",
    "* Prune the trained YOLO v4 Tiny model\n",
    "* Retrain the pruned model to recover lost accuracy\n",
    "* Export the pruned model\n",
    "* Run Inference on the trained model\n",
    "* Export the pruned and retrained model to a .onnx file for deployment on STM32 targets\n",
    "\n",
    "At the end of this notebook, you will have generated a trained and optimized `YOLOv4 Tiny` model\n",
    "which you can evaluate, quantize, benchmark, and deploy via [STEdgeAI Developer Cloud](https://stm32ai.st.com/stm32-cube-ai-dc/) and [stm32ai-modelzoo-services](https://github.com/STMicroelectronics/stm32ai-modelzoo-services).\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example use case of YOLO v4 Tiny object detection using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Install the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and pre-trained model](#head-2) <br>\n",
    "     2.1 [Download the dataset](#head-2-1)<br>\n",
    "     2.2 [Verify the downloaded dataset](#head-2-2)<br>\n",
    "     2.3 [Generate tfrecords](#head-2-3)<br>\n",
    "     2.4 [Download pretrained model](#head-2-4)\n",
    "3. [Provide training specification](#head-3)\n",
    "4. [Run TAO training](#head-4)\n",
    "5. [Evaluate trained models](#head-5)\n",
    "6. [Prune trained models](#head-6)\n",
    "7. [Retrain pruned models](#head-7)\n",
    "8. [Evaluate retrained model](#head-8)\n",
    "9. [Visualize inferences](#head-9)\n",
    "10. [Model Export](#head-10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/yolo_v4_tiny`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "*Note: Please make sure to remove any stray artifacts/files from the `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "# %env USER_EXPERIMENT_DIR=/workspace/tao-experiments/yolo_v4_tiny\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tao-experiments/yolo_v4_tiny\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tao-experiments/data\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/yolo_v4_tiny\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/yolo_v4_tiny\n",
    "# %env LOCAL_PROJECT_DIR=YOUR_LOCAL_PROJECT_DIR_PATH\n",
    "%env LOCAL_PROJECT_DIR=/local/home/stm32ai-tao/\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\")\n",
    "# os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"yolo_v4_tiny\")\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"yolo_v4_tiny\")\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")\n",
    "# %env SPECS_DIR=/workspace/tao-experiments/yolo_v4_tiny/specs\n",
    "%env SPECS_DIR=/workspace/tao-experiments/yolo_v4_tiny/specs\n",
    "\n",
    "# Showing list of specification files.\n",
    "!ls -rlt $LOCAL_SPECS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $LOCAL_DATA_DIR\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below maps the project directory on your local host to a workspace directory in the TAO docker instance, so that the data and the results are mapped from outside to inside of the docker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in PyPI. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.7, <=3.10.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n",
    "\n",
    "After setting up your virtual environment with the above requirements, install TAO pip package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install --upgrade nvidia-pyindex\n",
    "!pip3 install --upgrade nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info --verbose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using COCO2017 dataset for training an Yolo V4 Tiny model to detect persons in the image. \n",
    "\n",
    "The downloading and unzipping the dataset is long and out of the scope of this notebook. However, to make the process smoother for the users we provide the guidelines for data prepration below:\n",
    "\n",
    "Download the coco2017 dataset from the link [coco2017 Dataset Download](https://cocodataset.org/#download). In particular download \n",
    "* 2017 Train images \\[118K/18GB\\]\n",
    "* 2017 Val images \\[5K/1GB\\], and\n",
    "* 2017 Train/Val annotations \\[241MB\\]\n",
    "\n",
    "Unzip these files in a single directory called COCO2017. After unzipping, copy the python script file `./utils/generate_convert_coco_subset_to_kitti.py` to this folder. you should have a structure like below:\n",
    "```bash\n",
    "coco2017/\n",
    "... train2017/\n",
    "...... 00000***.jpg\n",
    "...... 00000***.jpg\n",
    "...... 00000***.jpg\n",
    "\n",
    "... val2017/\n",
    "...... 00000***.jpg\n",
    "...... 00000***.jpg\n",
    "...... 00000***.jpg\n",
    "\n",
    "... annotations\n",
    "...... instances_val2017.json\n",
    "...... instances_val2017.json\n",
    "\n",
    "... generate_convert_coco_subset_to_kitti.py\n",
    "```\n",
    "\n",
    "Then launch two commands below to filter and convert the dataset to the input format of this script.\n",
    "\n",
    "    python generate_convert_coco_subset_to_kitti.py --source-image-dir ./val2017 --source-annotation-file ./annotations/instances_val2017.json --out-data-dir ./coco2017_person/val2017 --num-images <num_images_to_keep> --categories-to-keep person\n",
    "\n",
    "    python generate_convert_coco_subset_to_kitti.py --source-image-dir ./train2017 --source-annotation-file ./annotations/instances_train2017.json --out-data-dir ./coco2017_person/train2017 --num-images <num_images_to_keep> --categories-to-keep person\n",
    "    \n",
    "_if you want to keep all the images for the chosen classes, do not provide the variable `--num-images` in the call._\n",
    "\n",
    "The result will be a folder inside the coco2017 folder called `coco2017_person` with structure like:\n",
    "```bash\n",
    "coco2017_person\n",
    "... val2017\n",
    "...... annotations\n",
    "......... instances_val2017.json # filtered coco annotations for the person class only\n",
    "...... images\n",
    "......... 0000*****.jpg\n",
    "......... 0000*****.jpg\n",
    "...... kitti_annotations\n",
    "......... 0000*****.txt\n",
    "......... 0000*****.txt\n",
    "\n",
    "... train2017\n",
    "...... annotations\n",
    "......... instances_train2017.json # filtered coco annotations for the person class only\n",
    "...... images\n",
    "......... 0000*****.jpg\n",
    "......... 0000*****.jpg\n",
    "...... kitti_annotations\n",
    "......... 0000*****.txt\n",
    "......... 0000*****.txt\n",
    "```\n",
    "\n",
    "<b> Note: The names of the directories have to be exactly like this otherwise the notebook will not run without adaption to use the names that are resulted. </b>\n",
    "\n",
    "Once the directories are created, copy the directory `coco2017_person` in the `stm32ai_tao/data/` directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Check the dataset <a class=\"anchor\" id=\"head-2-1\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is prepared and copied, this next cell, will show the number of images in the train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "import os\n",
    "\n",
    "DATA_DIR = os.environ.get('LOCAL_DATA_DIR')\n",
    "num_training_images = len(os.listdir(os.path.join(DATA_DIR, \"coco2017_person/train2017/images\")))\n",
    "num_training_labels = len(os.listdir(os.path.join(DATA_DIR, \"coco2017_person/train2017/kitti_annotations/\")))\n",
    "num_testing_images = len(os.listdir(os.path.join(DATA_DIR, \"coco2017_person/val2017/images\")))\n",
    "num_testing_labels = len(os.listdir(os.path.join(DATA_DIR, \"coco2017_person/val2017/kitti_annotations/\")))\n",
    "print(\"Number of images in the train/val set. {}\".format(num_training_images))\n",
    "print(\"Number of labels in the train/val set. {}\".format(num_training_labels))\n",
    "print(\"Number of images in the test set. {}\".format(num_testing_images))\n",
    "print(\"Number of labels in the test set. {}\".format(num_testing_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a Running the following cell will generate the anchor shapes based on the data for the training set.\n",
    "Generate the anchor shapes and then copy them in the training spec files. The values available in the spec files are generated using all the images with the person annotations in them in coco2017.\n",
    "\n",
    "__Following cell is not needed to be run or the values to be copied to spec files if same dataset is being used__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use your own dataset, you will need to run the code below to generate the best anchor shape\n",
    "\n",
    "!tao model yolo_v4_tiny kmeans -l $DATA_DOWNLOAD_DIR/coco2017_person/train2017/kitti_annotations/ \\\n",
    "                         -i $DATA_DOWNLOAD_DIR/coco2017_person/train2017/images/ \\\n",
    "                         -n 6 \\\n",
    "                         -x 256 \\\n",
    "                         -y 256\n",
    "\n",
    "# x and y are the values of the height and width.\n",
    "# The anchor shape generated by this script is sorted. Write the first 3 into small_anchor_shape in the config spec\n",
    "# file. Write middle 3 into mid_anchor_shape. Write last 3 into big_anchor_shape."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generate tfrecords <a class=\"anchor\" id=\"head-2-3\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default YOLOv4 Tiny data format requires generation of TFRecords. To do this run the two following cells.\n",
    "\n",
    "__Note: we observe the TFRecords format sometimes results in CUDA error during evaluation. Setting `force_on_cpu` in `nms_config` to `true` can help prevent this problem.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model yolo_v4_tiny dataset_convert -d $SPECS_DIR/yolo_v4_tiny_tfrecords_person_train.txt \\\n",
    "                             -o $DATA_DOWNLOAD_DIR/yolo_v4_tiny_person/tfrecords/train \\\n",
    "                             -r $USER_EXPERIMENT_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model yolo_v4_tiny dataset_convert -d $SPECS_DIR/yolo_v4_tiny_tfrecords_person_val.txt \\\n",
    "                             -o $DATA_DOWNLOAD_DIR/yolo_v4_tiny_person/tfrecords/val \\\n",
    "                             -r $USER_EXPERIMENT_DIR/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Download pre-trained model <a class=\"anchor\" id=\"head-2-4\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tao/pretrained_object_detection:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_cspdarknet_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/pretrained_object_detection:cspdarknet_tiny \\\n",
    "                   --dest $LOCAL_EXPERIMENT_DIR/pretrained_cspdarknet_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_EXPERIMENT_DIR/pretrained_cspdarknet_tiny/pretrained_object_detection_vcspdarknet_tiny"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
    "* Augmentation parameters for on-the-fly data augmentation\n",
    "* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide pretrained model path\n",
    "!sed -i 's,EXPERIMENT_DIR,'\"$USER_EXPERIMENT_DIR\"',' $LOCAL_SPECS_DIR/yolo_v4_tiny_train_person.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_SPECS_DIR/yolo_v4_tiny_train_person.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* WARNING: training will take several hours or one day to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n",
    "!tao model yolo_v4_tiny train -e $SPECS_DIR/yolo_v4_tiny_train_person.txt \\\n",
    "                   -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
    "                   --gpus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To resume from checkpoint, please change pretrain_model_path to resume_model_path in config file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model for each epoch:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check the evaluation stats in the csv file and pick the model with highest eval accuracy.\n",
    "!cat $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned/yolov4_training_log_cspdarknet_tiny.csv\n",
    "%set_env EPOCH=080"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model yolo_v4_tiny evaluate -e $SPECS_DIR/yolo_v4_tiny_train_person.txt \\\n",
    "                      -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov4_cspdarknet_tiny_epoch_$EPOCH.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tao <task> export will fail if .onnx already exists. So we clear the export folder before tao <task> export\n",
    "# !rm -rf $LOCAL_EXPERIMENT_DIR/export\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/export\n",
    "# Generate .onnx file using tao container\n",
    "!tao model yolo_v4_tiny export -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov4_cspdarknet_tiny_epoch_$EPOCH.hdf5 \\\n",
    "                               -o $USER_EXPERIMENT_DIR/export/yolov4_cspdarknet_tiny_unpruned_epoch_$EPOCH.onnx \\\n",
    "                               -e $SPECS_DIR/yolo_v4_tiny_train_person.txt \\\n",
    "                               --target_opset 15 \\\n",
    "                               --gen_ds_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prune trained models <a class=\"anchor\" id=\"head-6\"></a>\n",
    "* Specify pre-trained model\n",
    "* Equalization criterion\n",
    "* Threshold for pruning\n",
    "* Output directory to store the model\n",
    "\n",
    "Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model. `0.7` in the block below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model yolo_v4_tiny prune -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov4_cspdarknet_tiny_epoch_$EPOCH.hdf5 \\\n",
    "                   -e $SPECS_DIR/yolo_v4_tiny_train_person.txt \\\n",
    "                   -o $USER_EXPERIMENT_DIR/experiment_dir_pruned/yolov4_cspdarknet_tiny_pruned.hdf5 \\\n",
    "                   -eq geometric_mean \\\n",
    "                   -pth 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retrain pruned models <a class=\"anchor\" id=\"head-7\"></a>\n",
    "* Model needs to be re-trained to bring back accuracy after pruning\n",
    "* Specify re-training specification\n",
    "* WARNING: training will take several hours or one day to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the retrain spec file. \n",
    "# Here we have updated the spec file to include the newly pruned model as a pretrained weights.\n",
    "!sed -i 's,EXPERIMENT_DIR,'\"$USER_EXPERIMENT_DIR\"',' $LOCAL_SPECS_DIR/yolo_v4_tiny_retrain_person.txt\n",
    "!cat $LOCAL_SPECS_DIR/yolo_v4_tiny_retrain_person.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining using the pruned model as pretrained weights \n",
    "!tao model yolo_v4_tiny train --gpus 1 \\\n",
    "                   -e $SPECS_DIR/yolo_v4_tiny_retrain_person.txt \\\n",
    "                   -r $USER_EXPERIMENT_DIR/experiment_dir_retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the newly retrained model.\n",
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check the evaluation stats in the csv file and pick the model with highest eval accuracy.\n",
    "!cat $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain/yolov4_training_log_cspdarknet_tiny.csv\n",
    "%set_env EPOCH=080"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate retrained model <a class=\"anchor\" id=\"head-8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model yolo_v4_tiny evaluate -e $SPECS_DIR/yolo_v4_tiny_retrain_person.txt \\\n",
    "                      -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_cspdarknet_tiny_epoch_$EPOCH.hdf5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize inferences <a class=\"anchor\" id=\"head-9\"></a>\n",
    "In this section, we run the `infer` tool to generate inferences on the trained models and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy some test images\n",
    "!mkdir -p $LOCAL_DATA_DIR/test_samples_person\n",
    "!cp $LOCAL_DATA_DIR/coco2017_person/val2017/images/00000000* $LOCAL_DATA_DIR/test_samples_person/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference for detection on n images\n",
    "!tao model yolo_v4_tiny inference -i $DATA_DOWNLOAD_DIR/test_samples_person \\\n",
    "                       -e $SPECS_DIR/yolo_v4_tiny_retrain_person.txt \\\n",
    "                       -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_cspdarknet_tiny_epoch_$EPOCH.hdf5 \\\n",
    "                       -r $USER_EXPERIMENT_DIR/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inference` tool produces two outputs. \n",
    "1. Overlain images in `$LOCAL_EXPERIMENT_DIR/images_annotated`\n",
    "2. Frame by frame bbox labels in kitti format located in `$LOCAL_EXPERIMENT_DIR/labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "# !pip3 install \"matplotlib>=3.3.3, <4.0\"\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "OUTPUT_PATH = 'images_annotated' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 5 # number of columns in the visualizer grid.\n",
    "IMAGES = 25 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Export <a class=\"anchor\" id=\"head-10\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained a non-QAT model, you may export in FP32, FP16 or INT8 mode using the code block below. For INT8, you need to provide calibration image directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tao <task> export will fail if .onnx already exists. So we clear the export folder before tao <task> export\n",
    "# !rm -rf $LOCAL_EXPERIMENT_DIR/export\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/export\n",
    "# Generate .onnx file using tao container\n",
    "!tao model yolo_v4_tiny export -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov4_cspdarknet_tiny_epoch_$EPOCH.hdf5 \\\n",
    "                               -o $USER_EXPERIMENT_DIR/export/yolov4_cspdarknet_tiny_epoch_$EPOCH.onnx \\\n",
    "                               -e $SPECS_DIR/yolo_v4_tiny_retrain_person.txt \\\n",
    "                               --target_opset 15 \\\n",
    "                               --gen_ds_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and using the trained and exported model\n",
    "\n",
    "After running this cell the trained model is available as `.onnx` format in `./export/`. This model can then be used with the `stm32ai-modelzoo-services` to be \n",
    "- quantized\n",
    "- used to run inference\n",
    "- benchmarked, and\n",
    "- deployment on STM32NPU.\n",
    "\n",
    "However, the exported model has a post processing node as shown below, this post processing layer has to be removed before the model can be used with [stm32ai-modelzoo-services](https://github.com/STMicroelectronics/stm32ai-modelzoo-services/tree/main) or [STEdgeAI](https://stm32ai.st.com/stm32-cube-ai/).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"float: center;background-color: white; width: 1080\" src=\"../docs/post_processing_node_yolov4_tiny.png\" width=\"1080\">\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "To remove this post-processing layer please use the `./utils/remove_nms.py`.\n",
    "\n",
    "After removal of the post-processing node the model has two outputs called `box` and `cls` as below.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"float: center;background-color: white; width: 1080\" src=\"../docs/removed_nms_head.png\" width=\"1080\">\n",
    "\n",
    "<br> \n",
    "\n",
    "**Note**: The values shown as the shapes of the `cls` and `box` are when the input shape is 256 x 256 and batch_size of 1.\n",
    "\n",
    "\n",
    "To do this we need to install the python packages\n",
    "- onnx_graphsurgeon\n",
    "- numpy\n",
    "- onnx\n",
    "- onnxruntime\n",
    "\n",
    "Then correct the path to the model you want to remove the nms from in the file:\n",
    "```python\n",
    "input_model = './export/yolov4_cspdarknet_tiny_epoch_080.onnx' # correct the path\n",
    "```\n",
    "and launch the script \n",
    "> python remove_nms.py\n",
    "\n",
    "This will result in a model file `./export/yolov4_cspdarknet_tiny_epoch_080_no_nms.onnx`. This model file then can be used to run with the [using_yolo_v4_tiny_with_stm32ai_modelzoo.ipynb](./using_yolo_v4_tiny_with_stm32ai_modelzoo.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tao_launcher_v5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
