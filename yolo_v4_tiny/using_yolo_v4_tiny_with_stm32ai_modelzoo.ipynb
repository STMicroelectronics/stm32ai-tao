{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32339927",
   "metadata": {},
   "source": [
    "## YOLO_v4_tiny Object detection using STM32AI ModelZoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d16c36",
   "metadata": {},
   "source": [
    "``` python\n",
    "# /*---------------------------------------------------------------------------------------------\n",
    "#  * Copyright (c) 2022 STMicroelectronics.\n",
    "#  * All rights reserved.\n",
    "#  *\n",
    "#  * This software is licensed under terms that can be found in the LICENSE file in\n",
    "#  * the root directory of this software component.\n",
    "#  * If no LICENSE file comes with this software, it is provided AS-IS.\n",
    "#  *--------------------------------------------------------------------------------------------*/\n",
    "```\n",
    "\n",
    "### <u>NOTE: This notebook is tested with Python version 3.10.x.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22fa8b",
   "metadata": {},
   "source": [
    "## 0. Removing Post-Processing layer of exported model\n",
    "\n",
    "After training, optimizing and adapting the YOLO_v4_tiny Object Detection model using Jupyter Notebook [yolo_v4_tiny.ipynb](./yolo_v4_tiny.ipynb), the trained model is available as `.onnx` format in `./export/`. This model can then be used with the `stm32ai-modelzoo-services` to be \n",
    "- quantized\n",
    "- used to run inference\n",
    "- benchmarked, and\n",
    "- deployed on STM32NPU.\n",
    "\n",
    "However, the exported model has a post processing node as shown below, this post processing layer has to be removed before the model can be used with [stm32ai-modelzoo-services](https://github.com/STMicroelectronics/stm32ai-modelzoo-services/tree/main) or [STEdgeAI](https://stm32ai.st.com/stm32-cube-ai/).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"float: center;background-color: white; width: 1080\" src=\"../docs/post_processing_node_yolov4_tiny.png\" width=\"1080\">\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "To remove this post-processing layer please use the `./utils/remove_nms.py`.\n",
    "\n",
    "After removal of the post-processing node the model has two outputs called `box` and `cls` as below.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"float: center;background-color: white; width: 1080\" src=\"../docs/removed_nms_head.png\" width=\"1080\">\n",
    "\n",
    "<br> \n",
    "\n",
    "**Note**: The values shown as the shapes of the `cls` and `box` are when the input shape is `256 x 256` and batch_size of 1.\n",
    "\n",
    "\n",
    "To do this we need to install the python packages\n",
    "- onnx_graphsurgeon\n",
    "- numpy\n",
    "- onnx\n",
    "- onnxruntime\n",
    "\n",
    "Then correct the path to the model you want to remove the nms from in the file:\n",
    "```python\n",
    "input_model = '../export/yolov4_cspdarknet_tiny_epoch_***.onnx' # correct the path\n",
    "```\n",
    "and launch the scripts `remove_nms.py`. The following cell creates a python virtual environment and does all these steps. Running this cell will result in a model file `../export/yolov4_cspdarknet_tiny_epoch_***_no_nms.onnx`. This model file then can be used to run with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.chdir(\"utils\")\n",
    "venv_name = \"temp\"\n",
    "subprocess.run([\"python\", \"-m\", \"venv\", venv_name])\n",
    "subprocess.run([os.path.join(venv_name, \"Scripts\", \"pip\"), \"install\", \"onnx_graphsurgeon\", \"onnxruntime\", \"onnx\", \"numpy\"])\n",
    "subprocess.run([os.path.join(venv_name, \"Scripts\", \"python\"), \"remove_nms.py\"])\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaff98e",
   "metadata": {},
   "source": [
    "The cells below show how this model then can be used with [stm32ai-modelzoo-services](https://github.com/STMicroelectronics/stm32ai-modelzoo-services/tree/main).\n",
    "\n",
    "\n",
    "The rest of the notebook is arranged as below:\n",
    "\n",
    "<div style=\"border-bottom: 3px solid #273B5F\">\n",
    "<h2>Table of content</h2>\n",
    "<ul style=\"list-style-type: none\">\n",
    "  <li><a href=\"#Setup\">1. Setup Instructions</a></li>\n",
    "\n",
    "<li><a href=\"#Prep\">2. Preparing the Baseline Model</a></li>\n",
    "    <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#Prediction\">2.1 Prediction</a></li>\n",
    "    <li><a href=\"#Quantization\">2.2 Quantization</a></li>\n",
    "    <li><a href=\"#Prediction_2\">2.3 Prediction</a></li>\n",
    "    <li><a href=\"#Benchmarking\">2.4 Benchmarking the Model</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afa1b2",
   "metadata": {},
   "source": [
    "<div id=\"Setup\">\n",
    "    <h2>1. Setup Instructions</h2>\n",
    "</div>\n",
    "\n",
    "In this notebook, we present how to quantize, run prediction and benchmark a [YOLO_v4-tiny](https://docs.nvidia.com/tao/tao-toolkit-archive/tao-30-2202/text/object_detection/yolo_v4_tiny.html) object detection model on a STM32 board using the [STM32 model zoo](https://github.com/STMicroelectronics/stm32ai-modelzoo/tree/main). \n",
    "\n",
    "The STM32 model zoo contains valuable resource, is accessible on GitHub, and offers a range of use cases such as image classification, object detection, audio event detection, hand posture, and human activity recognition. It provides numerous services, including training, evaluation, prediction, deployment, quantization, benchmarking, and chained services, such as chain_tbqeb, chain_tqe, chain_eqe, chain_qb, chain_eqeb, and chain_qd, which are thoroughly explained in their respective readmes.\n",
    "\n",
    "To get started, you'll need to clone the stm32ai model zoo repository by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/STMicroelectronics/stm32ai-modelzoo-services.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54ca5a",
   "metadata": {},
   "source": [
    "After running the code above, navigate to the stm32ai-modelzoo-services repository and install the required libraries by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d09606",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('stm32ai-modelzoo-services')\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17cdcf",
   "metadata": {},
   "source": [
    "In this notebook, we will be utilizing the various services of the object detection service. To do so, we must navigate to the object detection source by running the code section below and use the `stm32ai_main.py` script in conjunction with a YAML file in the next sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('object_detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3f7dd",
   "metadata": {},
   "source": [
    "<div id=\"Prep\">\n",
    "    <h2>2. Preparing the Model</h2>\n",
    "</div>\n",
    "\n",
    "<div id=\"Prediction\">\n",
    "    <h3>2.1 Prediction</h3>\n",
    "</div>\n",
    "\n",
    "In this section, we will be using an object detection model and predict images. To achieve this, we will be using the `prediction_config.yaml` file located in `object_detection/src/config_file_examples` to specify the `operation_mode` and the other configuration parameters such as the `model_path`, `model_type`, `preprocessing`, `postprocessing` etc. \n",
    "\n",
    "To start off, change the path to the test directory, the model path and type and the operation mode to `prediction`.\n",
    "``` yaml\n",
    "general:\n",
    "   model_path: ../../export/yolov4_cspdarknet_tiny_epoch_***_no_nms.onnx\n",
    "   model_type: yolo_v4_tiny\n",
    "\n",
    "operation_mode: prediction\n",
    "```\n",
    "Then, make sure to modify preprocessing and postprocessing parameters in the configuration file to correspond to the parameters used during training:\n",
    "\n",
    "```yaml\n",
    "preprocessing:\n",
    "  rescaling: \n",
    "    scale: [1, 1, 1]\n",
    "    offset: [-103.939,-116.779,-123.68]\n",
    "  resizing:\n",
    "    aspect_ratio: fit\n",
    "    interpolation: bilinear\n",
    "  color_mode: bgr\n",
    "\n",
    "postprocessing:\n",
    "  confidence_thresh: 0.4 # to not have boxes with small confidence\n",
    "  NMS_thresh: 0.5\n",
    "  IoU_eval_thresh: 0.5\n",
    "  plot_metrics: True   # Plot precision versus recall curves. Default is False.\n",
    "  max_detection_boxes: 200\n",
    "```\n",
    "\n",
    "Finally, make sure to change the path to the directory containing the prediction images.\n",
    "```yaml\n",
    "prediction:\n",
    "  test_files_path: ../../../data/test_samples_person\n",
    "```\n",
    "\n",
    "Once this is done, you can run your predictions using the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19122c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path src/config_file_examples --config-name prediction_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa5021",
   "metadata": {},
   "source": [
    "<div id=\"Quantization\">\n",
    "    <h3>2.2 Quantization</h3>\n",
    "</div>\n",
    "\n",
    "In this section, we will quantize the float32 model to an int8 quantized model. Quantization is a technique used to reduce the memory and computation requirements of a model by converting the weights and activations from float32 to int8.\n",
    "\n",
    "To perform quantization, we will use the sample configuration file `quantization_config.yaml` provided in `object_detection/src/config_example_files` for providing the configurations. The configuration file specifies the `quantization_dataset_path`, `preprocessing`, the quantization parameters, such as the `quantization_input_type` and `quantization_output_type`. \n",
    "\n",
    "Depending on your needs, you can adapt the following parameters:\n",
    "\n",
    "``` yaml\n",
    "general:\n",
    "   model_path: ../../export/yolov4_cspdarknet_tiny_epoch_***_no_nms.onnx\n",
    "   model_type: yolo_v4_tiny\n",
    "\n",
    "operation_mode: quantization\n",
    "\n",
    "dataset:\n",
    "  name: COCO_2017_person\n",
    "  class_names: [ person ]\n",
    "  quantization_path: ../../../data/test_samples_person # containing sample images for the quantization (ideally the whole training set but few 20s will suffice)\n",
    "  quantization_split: 0.99 # to use all the images for the quantization\n",
    "\n",
    "preprocessing:\n",
    "  rescaling: \n",
    "    scale: [1, 1, 1]\n",
    "    offset: [-103.939,-116.779,-123.68]\n",
    "  resizing:\n",
    "    aspect_ratio: fit\n",
    "    interpolation: bilinear\n",
    "  color_mode: bgr\n",
    "\n",
    "postprocessing:\n",
    "  confidence_thresh: 0.4 # to not have boxes with small confidence\n",
    "  NMS_thresh: 0.5\n",
    "  IoU_eval_thresh: 0.5\n",
    "  plot_metrics: True   # Plot precision versus recall curves. Default is False.\n",
    "  max_detection_boxes: 200\n",
    "\n",
    "quantization:\n",
    "  quantizer: onnx_quantizer\n",
    "  target_opset: 17\n",
    "  granularity: per_channel #per_channel\n",
    "  quantization_type: PTQ\n",
    "  quantization_input_type: float \n",
    "  quantization_output_type: float\n",
    "  export_dir: quantized_models\n",
    "```\n",
    "\n",
    "After running the `stm32ai_main.py` script with the `quantization_config.yaml` file, an int8 quantized tflite model will be generate and saved under **experiments_outputs/experiment_runtime/quantized_models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54524492",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path src/config_file_examples --config-name quantization_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bae177",
   "metadata": {},
   "source": [
    "<div id=\"Prediction_2\">\n",
    "    <h3>2.3 Prediction</h3>\n",
    "</div>\n",
    "\n",
    "After the quantization of the model we can run the inference again to check that quantization has not effected the performance of the model. In this section, we will be using the quantized model for running the prediction on the test images.\n",
    "\n",
    "You should be using the same parameters as the previous prediction, but change the model path to the new model.\n",
    "\n",
    "```yaml\n",
    "general:\n",
    "    model_path: src/experiments_outputs/YYYY_MM_DD_HH_MM_SS/quantized_models/yolov4_cspdarknet_tiny_epoch_***_no_nms_quant_qdq_pc.onnx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path src/config_file_examples --config-name prediction_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4add5",
   "metadata": {},
   "source": [
    "<div id=\"Benchmarking\">\n",
    "    <h3>2.4 Benchmarking the Model</h2>\n",
    "</div>\n",
    "\n",
    "In this section we use the [STM32Cube.AI Developer Cloud](https://stm32ai-cs.st.com/home) to benchmark the quantized model on the **STM32N6750-DK** board.\n",
    "\n",
    "If you are behind a proxy, you can uncomment and fill the following proxy settings.\n",
    "\n",
    "**NOTE** : If the password contains some special characters like `@`, `:` etc. they need to be url-encoded with their ASCII values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636039f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['http_proxy'] = \"http://user:passwd@ip_address:port\"\n",
    "# os.environ['https_proxy'] = \"https://user:passwd@ip_address:port\"\n",
    "# And eventually disable SSL verification\n",
    "# os.environ['NO_SSL_VERIFY'] = \"1\"\n",
    "# os.environ[\"SSL_VERIFY\"]=\"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f8733",
   "metadata": {},
   "source": [
    "\n",
    "Set environment variables with your credentials to access STM32Cube.AI Developer Cloud. If you don't have an account yet go to : https://stm32ai-cs.st.com/home and click on sign in to create an account. Then set the environment variables below with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "email ='xxx.yyy@st.com'\n",
    "os.environ['stmai_username'] = email\n",
    "print('Enter your password')\n",
    "password = getpass.getpass()\n",
    "os.environ['stmai_password'] = password\n",
    "os.environ['NO_SSL_VERIFY'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5fe18",
   "metadata": {},
   "source": [
    "We will be using the `benchmarking_config.yaml` file in `object_detection/src/config_example_files/` to measure the performance of the baseline model on the **STM32N6750-DK**.\n",
    "\n",
    "```yaml\n",
    "general:\n",
    "    model_path: src/experiments_outputs/YYYY_MM_DD_HH_MM_SS/quantized_models/yolov4_cspdarknet_tiny_epoch_***_no_nms_quant_qdq_pc.onnx\n",
    "\n",
    "operation_mode: benchmarking\n",
    "\n",
    "tools:\n",
    "   stedgeai:\n",
    "      version: 10.0.0\n",
    "      optimization: balanced\n",
    "      on_cloud: True\n",
    "      path_to_stedgeai: path_to_local_installation_dir_of_stedgeai/stedgeai.exe #only needed if benchmarking is done locally.\n",
    "   path_to_cubeIDE: path_to_installations_dir_of_STM32CubeIDE/stm32cubeide.exe\n",
    "\n",
    "benchmarking:\n",
    "   board: STM32N6570-DK\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a217f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run stm32ai_main.py --config-path src/config_file_examples --config-name benchmarking_config.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
